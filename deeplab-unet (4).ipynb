{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906}],"dockerImageVersionId":30474,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport nibabel as nib\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, ModelCheckpoint)\nfrom tensorflow.keras.layers import *","metadata":{"execution":{"iopub.status.busy":"2023-11-15T00:30:18.496431Z","iopub.execute_input":"2023-11-15T00:30:18.496759Z","iopub.status.idle":"2023-11-15T00:30:26.879509Z","shell.execute_reply.started":"2023-11-15T00:30:18.496733Z","shell.execute_reply":"2023-11-15T00:30:26.878405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATASET_PATH = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\nVALIDATION_DATASET_PATH = '../input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/'\n# DEFINE seg-areas  \nSEGMENT_CLASSES = {\n    0 : 'NOT tumor',\n    1 : 'NECROTIC/CORE', # or NON-ENHANCING tumor CORE\n    2 : 'EDEMA',\n    3 : 'ENHANCING' # original 4 -> converted into 3 later\n}\nIMG_SIZE=128\n# there are 155 slices per volume\n# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \nVOLUME_SLICES = 100 \nVOLUME_START_AT = 22 # first slice of volume that we will include","metadata":{"execution":{"iopub.status.busy":"2023-11-15T00:30:26.881451Z","iopub.execute_input":"2023-11-15T00:30:26.882101Z","iopub.status.idle":"2023-11-15T00:30:26.887497Z","shell.execute_reply.started":"2023-11-15T00:30:26.882071Z","shell.execute_reply":"2023-11-15T00:30:26.886655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_imgs(paths, i):\n    sub_path = sorted(os.listdir(TRAIN_DATASET_PATH + paths[i]))\n    \n    image_flair = nib.load(TRAIN_DATASET_PATH + paths[i]+ '/' +sub_path[0]).get_fdata()\n    mask = nib.load(TRAIN_DATASET_PATH + paths[i]+ '/' + sub_path[1]).get_fdata()\n    image_t1 = nib.load(TRAIN_DATASET_PATH + paths[i]+ '/' + sub_path[2]).get_fdata()\n    image_t1ce = nib.load(TRAIN_DATASET_PATH + paths[i]+ '/' + sub_path[3]).get_fdata()\n    image_t2 = nib.load(TRAIN_DATASET_PATH + paths[i]+ '/' + sub_path[4]).get_fdata()\n    \n    print(f\"Height of the image: {image_flair.shape[0]}\")\n    print(f\"width of the image: {image_flair.shape[1]}\")\n    print(f\"number of slices of volume of the image: {image_flair.shape[-1]}\")\n    print()\n    \n    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize = (20, 10))\n    slice_w = 25\n    ax1.imshow(image_flair[:,:,image_flair.shape[0]//2-slice_w], cmap = 'gray')\n    ax1.set_title('Image flair')\n    ax1.axis(False)\n    ax2.imshow(image_t1[:,:,image_t1.shape[0]//2-slice_w], cmap = 'gray')\n    ax2.set_title('Image t1')\n    ax2.axis(False)\n    ax3.imshow(image_t1ce[:,:,image_t1ce.shape[0]//2-slice_w], cmap = 'gray')\n    ax3.set_title('Image t1ce')\n    ax3.axis(False)\n    ax4.imshow(image_t2[:,:,image_t2.shape[0]//2-slice_w], cmap = 'gray')\n    ax4.set_title('Image t2')\n    ax4.axis(False)\n    ax5.imshow(image_flair[:,:,image_flair.shape[0]//2-slice_w], cmap=\"OrRd\", alpha=1.0)\n    ax5.imshow(mask[:,:,mask.shape[0]//2-slice_w], alpha=0.2, cmap=\"OrRd\")\n    ax5.set_title('Mask')\n    ax5.axis(False)\n    print()\n#     plt.imshow(mask, cmap='reds')\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-15T00:30:26.892122Z","iopub.execute_input":"2023-11-15T00:30:26.892394Z","iopub.status.idle":"2023-11-15T00:30:26.906363Z","shell.execute_reply.started":"2023-11-15T00:30:26.892371Z","shell.execute_reply":"2023-11-15T00:30:26.905468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = sorted(os.listdir(r\"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"))\nshow_imgs(path, 0) # with Tumor\npath = sorted(os.listdir(r\"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"))\nshow_imgs(path, 0) # with Tumor","metadata":{"execution":{"iopub.status.busy":"2023-11-15T00:32:12.290815Z","iopub.execute_input":"2023-11-15T00:32:12.291535Z","iopub.status.idle":"2023-11-15T00:32:14.152401Z","shell.execute_reply.started":"2023-11-15T00:32:12.291495Z","shell.execute_reply":"2023-11-15T00:32:14.151530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n\n# file BraTS20_Training_355 has ill formatted name for for seg.nii file\ntrain_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n\n\ndef pathListIntoIds(dirList):\n    x = []\n    for i in range(0,len(dirList)):\n        x.append(dirList[i][dirList[i].rfind('/')+1:])\n    return x\n\ntrain_and_test_ids = pathListIntoIds(train_and_val_directories); \n\n    \ntrain_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) \ntrain_ids, test_ids = train_test_split(train_test_ids,test_size=0.15) ","metadata":{"execution":{"iopub.status.busy":"2023-11-15T00:30:58.293298Z","iopub.execute_input":"2023-11-15T00:30:58.294216Z","iopub.status.idle":"2023-11-15T00:30:58.430830Z","shell.execute_reply.started":"2023-11-15T00:30:58.294178Z","shell.execute_reply":"2023-11-15T00:30:58.430058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.applications.MobileNetV2(input_shape=(128,128,3), include_top=False, weights='imagenet').summary","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:06:00.432793Z","iopub.execute_input":"2023-05-18T11:06:00.433292Z","iopub.status.idle":"2023-05-18T11:06:05.667019Z","shell.execute_reply.started":"2023-05-18T11:06:00.433260Z","shell.execute_reply":"2023-05-18T11:06:05.665638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_data(list_ids, batch_size, img_size):\n    while True:\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        indexes = np.arange(len(list_ids))\n    #     indexes = indexes[index*batch_size:(index+1)*batch_size]\n        np.random.shuffle(indexes)\n        Batch_ids = [list_ids[k] for k in indexes[:batch_size]]\n\n        # Initialization\n        X = np.zeros((batch_size*VOLUME_SLICES, img_size[0], img_size[1], img_size[2]))\n        y = np.zeros((batch_size*VOLUME_SLICES, 240, 240))\n        Y = np.zeros((batch_size*VOLUME_SLICES, img_size[0], img_size[1], 4))\n\n\n        # Generate data\n        for c, i in enumerate(Batch_ids):\n            case_path = os.path.join(TRAIN_DATASET_PATH, i)\n\n            data_path = os.path.join(case_path, f'{i}_flair.nii')\n            flair = nib.load(data_path).get_fdata()    \n\n            data_path = os.path.join(case_path, f'{i}_t1ce.nii')\n            ce = nib.load(data_path).get_fdata()\n            \n            data_path = os.path.join(case_path, f'{i}_seg.nii')\n            seg = nib.load(data_path).get_fdata()\n\n            for j in range(VOLUME_SLICES):\n                X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))   \n                X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n                y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT]\n\n        # Generate masks\n        y[y==4] = 3\n        mask = tf.one_hot(y, len(SEGMENT_CLASSES))\n        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE))\n        yield X/np.max(X), Y\n        \n\ndef data_generator_wrapper(list_ids, batch_size=1, img_size=(IMG_SIZE, IMG_SIZE, 2)):\n    if len(list_ids)==0 or batch_size<=0: return None\n    return generate_data(list_ids, batch_size, img_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:06:05.672095Z","iopub.execute_input":"2023-05-18T11:06:05.672567Z","iopub.status.idle":"2023-05-18T11:06:05.694938Z","shell.execute_reply.started":"2023-05-18T11:06:05.672534Z","shell.execute_reply":"2023-05-18T11:06:05.693943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_coef(y_true, y_pred, smooth=1.0):\n    class_num = 4\n    for i in range(class_num):\n        y_true_f = K.flatten(y_true[:,:,:,i])\n        y_pred_f = K.flatten(y_pred[:,:,:,i])\n        intersection = K.sum(y_true_f * y_pred_f)\n        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n        if i == 0:\n            total_loss = loss\n        else:\n            total_loss = total_loss + loss\n    total_loss = total_loss / class_num\n    return total_loss\n\n\ndef dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)\n\ndef dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)\n\ndef dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)\n\n\ndef precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\n    \ndef sensitivity(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    return true_positives / (possible_positives + K.epsilon())\n\n\ndef specificity(y_true, y_pred):\n    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n    return true_negatives / (possible_negatives + K.epsilon())","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:06:05.699142Z","iopub.execute_input":"2023-05-18T11:06:05.699497Z","iopub.status.idle":"2023-05-18T11:06:05.726754Z","shell.execute_reply.started":"2023-05-18T11:06:05.699466Z","shell.execute_reply":"2023-05-18T11:06:05.725626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def DeepLabV3Plus_UNet(input_shape, num_classes):\n    # U-Net architecture\n    def UNet(input_tensor):\n        # Downsample path\n        conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_tensor)\n        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n        \n        conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n        \n        conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n        \n        conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n        pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n        \n        # Bridge\n        conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n        conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n\n        # Upsample path\n        up6 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5)\n        concat6 = Concatenate()([up6, conv4])\n        conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(concat6)\n        \n        up7 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6)\n        concat7 = Concatenate()([up7, conv3])\n        conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(concat7)\n        \n        up8 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7)\n        concat8 = Concatenate()([up8, conv2])\n        conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(concat8)\n        \n        up9 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8)\n        concat9 = Concatenate()([up9, conv1])\n        conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(concat9)\n        \n        return conv9\n    \n    # DeepLabv3+ architecture\n    def DeepLabV3Plus(input_tensor):\n        # DeepLabv3+ implementation (without ASPP module)\n        # Modify this part to include ASPP module\n        x = Conv2D(filters=3,kernel_size=3,  padding='same', activation='relu')(input_tensor)\n        # Backbone (e.g., ResNet, MobileNetV2)\n        backbone = tf.keras.applications.MobileNetV2(input_shape=(128,128,3), include_top=False, weights='imagenet')\n        backbone_output = backbone(x)\n        \n        # Upsampling\n        upsample1 = UpSampling2D((4, 4))(backbone_output)\n        \n        # U-Net path\n        unet_output = UNet(upsample1)\n        \n        # upsampling and Convolutional layers\n        upsample2 = UpSampling2D((8, 8))(unet_output)\n        x = Conv2D(64, (3, 3), padding='same', activation='relu')(upsample2)\n        x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n        \n        # Final prediction\n        output = Conv2D(num_classes, (1, 1), activation='softmax')(x)\n        \n        return output\n    \n    # Input tensor\n    input_tensor = Input(shape=input_shape)\n    \n    \n    # DeepLabv3+ with U-Net\n    deep_lab_unet_output = DeepLabV3Plus(input_tensor)\n    \n    # Create the model\n    return tf.keras.Model(inputs=input_tensor, outputs=deep_lab_unet_output)","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:06:05.731415Z","iopub.execute_input":"2023-05-18T11:06:05.731744Z","iopub.status.idle":"2023-05-18T11:06:05.758819Z","shell.execute_reply.started":"2023-05-18T11:06:05.731715Z","shell.execute_reply":"2023-05-18T11:06:05.757639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = (IMG_SIZE,IMG_SIZE, 2)\nnum_classes = len(SEGMENT_CLASSES)\nmodel_2 = DeepLabV3Plus_UNet(input_shape, num_classes)\nmodel_2.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:06:05.762165Z","iopub.execute_input":"2023-05-18T11:06:05.762677Z","iopub.status.idle":"2023-05-18T11:06:07.887223Z","shell.execute_reply.started":"2023-05-18T11:06:05.762646Z","shell.execute_reply":"2023-05-18T11:06:07.886462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\n# Plot the model structure\ntf.keras.utils.plot_model(model_2, to_file='deeplabv3plus_model.png', show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:06:07.888260Z","iopub.execute_input":"2023-05-18T11:06:07.888659Z","iopub.status.idle":"2023-05-18T11:06:08.316486Z","shell.execute_reply.started":"2023-05-18T11:06:07.888633Z","shell.execute_reply":"2023-05-18T11:06:08.315666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.compile(loss=\"categorical_crossentropy\", optimizer='adam', \n             metrics = ['accuracy', tf.keras.metrics.MeanIoU(num_classes=len(SEGMENT_CLASSES)), \n                        dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, \n                        dice_coef_edema ,dice_coef_enhancing])","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:06:08.317748Z","iopub.execute_input":"2023-05-18T11:06:08.318564Z","iopub.status.idle":"2023-05-18T11:06:08.348808Z","shell.execute_reply.started":"2023-05-18T11:06:08.318530Z","shell.execute_reply":"2023-05-18T11:06:08.347919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping_cb = EarlyStopping(patience=5, restore_best_weights=True, verbose=1)\ncheckpoints_cb = ModelCheckpoint(\"model_weights_vir.h5\", save_best_only=True, verbose=1)\nreducee_lr_cb = ReduceLROnPlateau(patience=3, verbose=1)\n\n# # TensorBoard Support\n# # tensorboard --logdir=./my_logs_vir --port=5050\n# root_logdir = os.path.join(os.curdir, \"my_logs_vir\")\n# def get_run_logdir():\n#     import time\n#     run_id = time.strftime(\"run_%Y_%m_%d-#H_%M_%S\")\n#     return os.path.join(root_logdir, run_id)\n# run_logdir = get_run_logdir()\n# tensorboard_cb = TensorBoard(run_logdir)\n\ncallbackss = [checkpoints_cb, reducee_lr_cb, early_stopping_cb]","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:06:08.350116Z","iopub.execute_input":"2023-05-18T11:06:08.350759Z","iopub.status.idle":"2023-05-18T11:06:08.357079Z","shell.execute_reply.started":"2023-05-18T11:06:08.350723Z","shell.execute_reply":"2023-05-18T11:06:08.356188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=1\nhistory = model_2.fit(data_generator_wrapper(train_ids, batch_size=batch_size), \n                    epochs=10,\n                    steps_per_epoch=max(1, len(train_ids)//batch_size), \n                    validation_data=data_generator_wrapper(val_ids, batch_size=batch_size), \n                    validation_steps=max(1, len(val_ids)//batch_size), \n                    initial_epoch=0, \n                    callbacks = callbackss)","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:06:08.358531Z","iopub.execute_input":"2023-05-18T11:06:08.358975Z","iopub.status.idle":"2023-05-18T11:33:45.859019Z","shell.execute_reply.started":"2023-05-18T11:06:08.358843Z","shell.execute_reply":"2023-05-18T11:33:45.856942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 2, figsize=(10, 10))\nax[0, 0].plot(history.history['loss'], label=\"Training Loss\")\nax[0, 0].plot(history.history['val_loss'], label='Validation Loss')\nax[0, 0].set_title('Loss')\nax[0, 0].legend()\n\nax[0, 1].plot(history.history['accuracy'], label=\"Training accuracy\")\nax[0, 1].plot(history.history['val_accuracy'], label='Validation accuracy')\nax[0, 1].set_title(\"Accuracy\")\nax[0, 1].legend()\n\nax[1, 0].plot(history.history['mean_io_u'], label=\"Training meanIOU\")\nax[1, 0].plot(history.history['val_mean_io_u'], label='Validation meanIOU')\nax[1, 0].set_title(\"Mean IOU\")\nax[1, 0].legend()\n\nax[1, 1].plot(history.history['dice_coef'], label=\"Training dice_coef\")\nax[1, 1].plot(history.history['val_dice_coef'], label='Validation dice_coef')\nax[1, 1].set_title(\"Dice Coefficient\")\nax[1, 1].legend()\n\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:33:53.222209Z","iopub.execute_input":"2023-05-18T11:33:53.222575Z","iopub.status.idle":"2023-05-18T11:33:54.387917Z","shell.execute_reply.started":"2023-05-18T11:33:53.222545Z","shell.execute_reply":"2023-05-18T11:33:54.387028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 3, figsize=(15, 8))\nax[0].plot(history.history['precision'], label=\"Training precision\")\nax[0].plot(history.history['val_precision'], label='Validation precision')\nax[0].set_title('Precision')\nax[0].legend()\n\nax[1].plot(history.history['sensitivity'], label=\"Training sensitivity\")\nax[1].plot(history.history['val_sensitivity'], label='Validation sensitivity')\nax[1].set_title(\"Sensitivity\")\nax[1].legend()\n\nax[2].plot(history.history['specificity'], label=\"Training specificity\")\nax[2].plot(history.history['val_specificity'], label='Validation specificity')\nax[2].set_title(\"Specificity\")\nax[2].legend()\n\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:34:00.639161Z","iopub.execute_input":"2023-05-18T11:34:00.640173Z","iopub.status.idle":"2023-05-18T11:34:02.287039Z","shell.execute_reply.started":"2023-05-18T11:34:00.640125Z","shell.execute_reply":"2023-05-18T11:34:02.286019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_tumors(case, start_slice = 60):\n    path = TRAIN_DATASET_PATH + \"/\" + case     \n    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n\n    flair = nib.load(path + \"/\" + case + \"_flair.nii\").get_fdata()\n    mask = nib.load(path + \"/\" + case + \"_seg.nii\").get_fdata()\n    ce = nib.load(path + \"/\" + case + \"_t1ce.nii\").get_fdata()\n\n    for j in range(VOLUME_SLICES):\n        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n        X[j,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n        \n    pred = model_2.predict(X/np.max(X), verbose=1)\n\n    core = pred[:,:,:,1]\n    edema= pred[:,:,:,2]\n    enhancing = pred[:,:,:,3]\n\n    f, ax = plt.subplots(2, 3) \n\n    for i in range(2): # for each image, add brain background\n        for j in range(3):\n            ax[i, j].imshow(cv2.resize(flair[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\", interpolation='none')\n    \n    ax[0, 0].imshow(cv2.resize(flair[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n    ax[0, 0].title.set_text('Original image flair')\n    \n    mask=cv2.resize(mask[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n    ax[0, 1].imshow(mask, cmap=\"Reds\", interpolation='none', alpha=0.3)\n    ax[0, 1].title.set_text('Ground truth')\n    \n    ax[0, 2].imshow(pred[start_slice,:,:,1:4], cmap=\"Reds\", interpolation='none', alpha=0.3)\n    ax[0, 2].title.set_text('all classes')\n    \n    ax[1, 0].imshow(edema[start_slice,:,:], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    ax[1, 0].title.set_text(f'{SEGMENT_CLASSES[1]} predicted')\n    \n    ax[1, 1].imshow(core[start_slice,:,:], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    ax[1, 1].title.set_text(f'{SEGMENT_CLASSES[2]} predicted')\n    \n    ax[1, 2].imshow(enhancing[start_slice,:,:], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    ax[1, 2].title.set_text(f'{SEGMENT_CLASSES[3]} predicted')\n    \n    plt.tight_layout()\n    plt.show()\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:34:51.276469Z","iopub.execute_input":"2023-05-18T11:34:51.277152Z","iopub.status.idle":"2023-05-18T11:34:51.293184Z","shell.execute_reply.started":"2023-05-18T11:34:51.277095Z","shell.execute_reply":"2023-05-18T11:34:51.292084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ids in np.random.choice(train_ids, size=5, replace=False):\n    predict_tumors(ids)","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:34:55.823726Z","iopub.execute_input":"2023-05-18T11:34:55.824091Z","iopub.status.idle":"2023-05-18T11:35:06.747071Z","shell.execute_reply.started":"2023-05-18T11:34:55.824062Z","shell.execute_reply":"2023-05-18T11:35:06.746104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Get the accuracy, loss, sensitivity, specificity, and precision from the history\naccuracy = history.history['accuracy']\nloss = history.history['loss']\nsensitivity = history.history['sensitivity']\nspecificity = history.history['specificity']\nprecision = history.history['precision']\n\n# Create a DataFrame\ndf = pd.DataFrame({'Epoch': range(1, len(accuracy) + 1), 'Accuracy': accuracy, 'Loss': loss, 'Sensitivity': sensitivity, 'Specificity': specificity, 'Precision': precision})\n\n# Print the DataFrame\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:35:11.362479Z","iopub.execute_input":"2023-05-18T11:35:11.362864Z","iopub.status.idle":"2023-05-18T11:35:11.395656Z","shell.execute_reply.started":"2023-05-18T11:35:11.362833Z","shell.execute_reply":"2023-05-18T11:35:11.394721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport nibabel as nib\nimport numpy as np\nfrom tabulate import tabulate\n\n# Directory paths where the NIfTI files are located\nnifti_dir_train = \"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001\"\nnifti_dir_val = \"/kaggle/input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/BraTS20_Validation_001/\"\n\n# Initialize a list to store the modalities and standard deviations\nmodalities_list = []\n\n# Iterate over the files in the training data directory\nfor file_name_train in os.listdir(nifti_dir_train):\n    # Check if the file is a NIfTI file\n    if file_name_train.endswith(\".nii\") or file_name_train.endswith(\".nii.gz\"):\n        # Get the file path\n        file_path_train = os.path.join(nifti_dir_train, file_name_train)\n        # Load the NIfTI file\n        nifti_img_train = nib.load(file_path_train)\n        # Get the image data array\n        image_data_train = nifti_img_train.get_fdata()\n        # Calculate the mean and standard deviation of the modalities\n        modalities_mean_train = np.mean(image_data_train, axis=(0, 1, 2))\n        modalities_std_train = np.std(image_data_train, axis=(0, 1, 2))\n        # Append the modalities and standard deviations to the list\n        modalities_list.append([file_name_train, modalities_mean_train, modalities_std_train])\n\n# Iterate over the files in the validation data directory\nfor file_name_val in os.listdir(nifti_dir_val):\n    # Check if the file is a NIfTI file\n    if file_name_val.endswith(\".nii\") or file_name_val.endswith(\".nii.gz\"):\n        # Get the file path\n        file_path_val = os.path.join(nifti_dir_val, file_name_val)\n        # Load the NIfTI file\n        nifti_img_val = nib.load(file_path_val)\n        # Get the image data array\n        image_data_val = nifti_img_val.get_fdata()\n        # Calculate the mean and standard deviation of the modalities\n        modalities_mean_val = np.mean(image_data_val, axis=(0, 1, 2))\n        modalities_std_val = np.std(image_data_val, axis=(0, 1, 2))\n        # Append the modalities and standard deviations to the list\n        modalities_list.append([file_name_val, modalities_mean_val, modalities_std_val])\n\n# Define the table headers\nheaders = [\"NIfTI File\", \"Mean\", \"Standard Deviation\"]\n\n# Print the table\nprint(tabulate(modalities_list, headers, tablefmt=\"grid\"))","metadata":{"execution":{"iopub.status.busy":"2023-05-18T11:35:16.844011Z","iopub.execute_input":"2023-05-18T11:35:16.844884Z","iopub.status.idle":"2023-05-18T11:35:18.770962Z","shell.execute_reply.started":"2023-05-18T11:35:16.844847Z","shell.execute_reply":"2023-05-18T11:35:18.770044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport nibabel as nib\nimport SimpleITK as sitk\n\n# Provide the path to your input image\nimage_path = \"/path/to/your/image.nii\"\n\n# Load the image\nimage = nib.load(image_path)\nimage_data = image.get_fdata()\n\n# Display slices of the image\nnum_slices = image_data.shape[-1]  # Number of slices in the image\nmid_slice = num_slices // 2  # Select the middle slice or adjust as needed\n\n# Display the selected slice\nplt.imshow(image_data[..., mid_slice], cmap='gray')\nplt.axis('on')\nplt.show()\n\n# Load the image using SimpleITK\nimage_sitk = sitk.ReadImage(image_path)\n\n# Apply thresholding to segment active tumor\nthreshold = sitk.BinaryThresholdImageFilter()\nthreshold.SetLowerThreshold(1)  # Adjust the threshold value as per your data\nthreshold.SetUpperThreshold(100)  # Adjust the threshold value as per your data\nthreshold.SetInsideValue(0)\nthreshold.SetOutsideValue(1)\nsegmented_image = threshold.Execute(image_sitk)\n\n# Optional: Apply morphological operations for refinement\nmorphology = sitk.BinaryMorphologicalOpeningImageFilter()\nmorphology.SetKernelRadius(2)  # Adjust the kernel radius as per your requirement\nsegmented_image = morphology.Execute(segmented_image)\n\n# Save the segmented active tumor image\noutput_path = \"segmented_image.nii.gz\"\nsitk.WriteImage(segmented_image, output_path)\n\n# Load the segmented active tumor image\nsegmented_image_data = nib.load(output_path).get_fdata()\n\n# Choose a slice to display (assuming a 2D image or selecting a slice from a 3D image)\nslice_index = 100  # Adjust the slice index as needed\n\n# Display the segmented active tumor image\nplt.imshow(segmented_image_data[:, :, slice_index], cmap='jet')\nplt.axis('on')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T12:10:28.175029Z","iopub.execute_input":"2023-05-25T12:10:28.175831Z","iopub.status.idle":"2023-05-25T12:10:29.407939Z","shell.execute_reply.started":"2023-05-25T12:10:28.175796Z","shell.execute_reply":"2023-05-25T12:10:29.405385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}